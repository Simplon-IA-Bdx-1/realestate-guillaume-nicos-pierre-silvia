{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "steps_dir = './pipeline_steps'\n",
    "cpu_cluster_name = \"cpucluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "# Create a new runconfig object\n",
    "run_amlcompute = RunConfiguration()\n",
    "\n",
    "# Use the cpu_cluster you created above. \n",
    "run_amlcompute.target = cpu_cluster\n",
    "\n",
    "# Enable Docker\n",
    "run_amlcompute.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "run_amlcompute.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_amlcompute.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "#run_amlcompute.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = Dataset.get_by_name(ws, name=\"annonces_ds\")\n",
    "\n",
    "clean_ds = PipelineData(\"dataset_clean\",\n",
    "                        datastore=def_blob_store)\n",
    "\n",
    "clean_step = PythonScriptStep(\n",
    "    script_name=\"clean.py\",\n",
    "    arguments=[\"--input\", dataset_full.name, \"--output\", clean_ds.name],\n",
    "    inputs=[dataset_full.as_named_input('annonce_ds')],\n",
    "    outputs=[clean_ds],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory=steps_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = Pipeline(workspace=ws, steps=[clean_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineOutputFileDataset' object has no attribute 'to_pandas_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1bb31be3a720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset_clean.register(\"fulltrain\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clean_ds = Dataset.get_by_name(ws, name=\"fulltrain\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclean_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineOutputFileDataset' object has no attribute 'to_pandas_dataframe'"
     ]
    }
   ],
   "source": [
    "dataset_clean = clean_ds.as_dataset()\n",
    "#dataset_clean.register(\"fulltrain\")\n",
    "#clean_ds = Dataset.get_by_name(ws, name=\"fulltrain\")\n",
    "clean_df = dataset_clean.to_pandas_dataframe()\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PipelineData in module azureml.pipeline.core.builder object:\n",
      "\n",
      "class PipelineData(builtins.object)\n",
      " |  PipelineData(name, datastore=None, output_name=None, output_mode='mount', output_path_on_compute=None, output_overwrite=None, data_type=None, is_directory=None, pipeline_output_name=None, training_output=None)\n",
      " |  \n",
      " |  Represents intermediate data in an Azure Machine Learning pipeline.\n",
      " |  \n",
      " |  Data used in pipeline can be produced by one step and consumed in another step by providing a PipelineData\n",
      " |  object as an output of one step and an input of one or more subsequent steps.\n",
      " |  \n",
      " |  .. remarks::\n",
      " |  \n",
      " |      PipelineData represents data output a step will produce when it is run. Use PipelineData when creating steps\n",
      " |      to describe the files or directories which will be generated by the step. These data outputs will be added to\n",
      " |      the specified Datastore and can be retrieved and viewed later.\n",
      " |  \n",
      " |      For example, the following pipeline step produces one output, named \"model\":\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from azureml.pipeline.core import PipelineData\n",
      " |          from azureml.pipeline.steps import PythonScriptStep\n",
      " |  \n",
      " |          datastore = ws.get_default_datastore()\n",
      " |          step_output = PipelineData(\"model\", datastore=datastore)\n",
      " |          step = PythonScriptStep(script_name=\"train.py\",\n",
      " |                                  arguments=[\"--model\", step_output],\n",
      " |                                  outputs=[step_output],\n",
      " |                                  compute_target=aml_compute,\n",
      " |                                  source_directory=source_directory)\n",
      " |  \n",
      " |      In this case, the train.py script will write the model it produces to the location which is provided to the\n",
      " |      script through the --model argument.\n",
      " |  \n",
      " |      PipelineData objects are also used when constructing Pipelines to describe step dependencies. To specify that\n",
      " |      a step requires the output of another step as input, use a PipelineData object in the constructor of both\n",
      " |      steps.\n",
      " |  \n",
      " |      For example, the pipeline train step depends on the process_step_output output of the pipeline process step:\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from azureml.pipeline.core import Pipeline, PipelineData\n",
      " |          from azureml.pipeline.steps import PythonScriptStep\n",
      " |  \n",
      " |          datastore = ws.get_default_datastore()\n",
      " |          process_step_output = PipelineData(\"processed_data\", datastore=datastore)\n",
      " |          process_step = PythonScriptStep(script_name=\"process.py\",\n",
      " |                                          arguments=[\"--data_for_train\", process_step_output],\n",
      " |                                          outputs=[process_step_output],\n",
      " |                                          compute_target=aml_compute,\n",
      " |                                          source_directory=process_directory)\n",
      " |          train_step = PythonScriptStep(script_name=\"train.py\",\n",
      " |                                        arguments=[\"--data_for_train\", process_step_output],\n",
      " |                                        inputs=[process_step_output],\n",
      " |                                        compute_target=aml_compute,\n",
      " |                                        source_directory=train_directory)\n",
      " |  \n",
      " |          pipeline = Pipeline(workspace=ws, steps=[process_step, train_step])\n",
      " |  \n",
      " |      This will create a Pipeline with two steps. The process step will be executed first, then after it has\n",
      " |      completed, the train step will be executed. Azure ML will provide the output produced by the process\n",
      " |      step to the train step.\n",
      " |  \n",
      " |      See this page for further examples of using PipelineData to construct a Pipeline: https://aka.ms/pl-data-dep\n",
      " |  \n",
      " |      For supported compute types, PipelineData can also be used to specify how the data will be produced and\n",
      " |      consumed by the run. There are two supported methods:\n",
      " |  \n",
      " |      * Mount (default): The input or output data is mounted to local storage on the compute\n",
      " |        node, and an environment variable is set which points to the path of this data ($AZUREML_DATAREFERENCE_name).\n",
      " |        For convenience, you can pass the PipelineData object in as one of the arguments to your script, for\n",
      " |        example using the ``arguments`` parameter of :class:`azureml.pipeline.steps.PythonScriptStep`, and\n",
      " |        the object will resolve to the path to the data. For outputs, your compute script should create a file\n",
      " |        or directory at this output path. To see the value of the environment variable used when you pass in the\n",
      " |        Pipeline object as an argument, use the :meth:`azureml.pipeline.core.PipelineData.get_env_variable_name`\n",
      " |        method.\n",
      " |  \n",
      " |      * Upload: Specify an ``output_path_on_compute`` corresponding to a file or directory\n",
      " |        name that your script will generate. (Environment variables are not used in this case.)\n",
      " |  \n",
      " |  \n",
      " |  :param name: The name of the PipelineData object, which can contain only letters, digits, and underscores.\n",
      " |  :type name: str\n",
      " |  :param datastore: The Datastore the PipelineData will reside on. If unspecified, the default datastore is used.\n",
      " |  :type datastore: azureml.data.azure_storage_datastore.AbstractAzureStorageDatastore or\n",
      " |      azureml.data.azure_data_lake_datastore.AzureDataLakeDatastore\n",
      " |  :param output_name: The name of the output, if None name is used. Can contain only letters, digits, and\n",
      " |      underscores.\n",
      " |  :type output_name: str\n",
      " |  :param output_mode: Specifies whether the producing step will use \"upload\" or \"mount\" method to access the data.\n",
      " |  :type output_mode: str\n",
      " |  :param output_path_on_compute: For ``output_mode`` = \"upload\", this parameter represents the path the\n",
      " |      module writes the output to.\n",
      " |  :type output_path_on_compute: str\n",
      " |  :param output_overwrite: For ``output_mode`` = \"upload\", this parameter specifies whether to overwrite\n",
      " |      existing data.\n",
      " |  :type output_overwrite: bool\n",
      " |  :param data_type: Optional. Data type can be used to specify the expected type of the output and to detail how\n",
      " |      consuming steps should use the data. It can be any user-defined string.\n",
      " |  :type data_type: str\n",
      " |  :param is_directory: Specifies whether the data is a directory or single file. This is only used to determine\n",
      " |      a data type used by Azure ML backend when the ``data_type`` parameter is not provided. The default is False.\n",
      " |  :type is_directory: bool\n",
      " |  :param pipeline_output_name: If provided this output will be available by using\n",
      " |      ``PipelineRun.get_pipeline_output()``. Pipeline output names must be unique in the pipeline.\n",
      " |  :param training_output: Defines output for training result. This is needed only for specific trainings which\n",
      " |                          result in different kinds of outputs such as Metrics and Model.\n",
      " |                          For example, :class:`azureml.train.automl.runtime.AutoMLStep` results in metrics and model.\n",
      " |                          You can also define specific training iteration or metric used to get best model.\n",
      " |  :type training_output: azureml.pipeline.core.TrainingOutput, optional\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name, datastore=None, output_name=None, output_mode='mount', output_path_on_compute=None, output_overwrite=None, data_type=None, is_directory=None, pipeline_output_name=None, training_output=None)\n",
      " |      Initialize PipelineData.\n",
      " |      \n",
      " |      :param name: The name of the PipelineData object, which can contain only letters, digits, and underscores.\n",
      " |      :type name: str\n",
      " |      :param datastore: The Datastore the PipelineData will reside on. If unspecified, the default datastore is used.\n",
      " |      :type datastore: azureml.data.azure_storage_datastore.AbstractAzureStorageDatastore or\n",
      " |          azureml.data.azure_data_lake_datastore.AzureDataLakeDatastore\n",
      " |      :param output_name: The name of the output, if None name is used. which can contain only letters, digits,\n",
      " |          and underscores.\n",
      " |      :type output_name: str\n",
      " |      :param output_mode: Specifies whether the producing step will use \"upload\" or \"mount\"\n",
      " |          method to access the data.\n",
      " |      :type output_mode: str\n",
      " |      :param output_path_on_compute: For ``output_mode`` = \"upload\", this parameter represents the path the\n",
      " |          module writes the output to.\n",
      " |      :type output_path_on_compute: str\n",
      " |      :param output_overwrite: For ``output_mode`` = \"upload\", this parameter specifies whether to overwrite\n",
      " |          existing data.\n",
      " |      :type output_overwrite: bool\n",
      " |      :param data_type: Optional. Data type can be used to specify the expected type of the output and to detail how\n",
      " |                        consuming steps should use the data. It can be any user-defined string.\n",
      " |      :type data_type: str\n",
      " |      :param is_directory: Specifies whether the data is a directory or single file. This is only used to determine\n",
      " |          a data type used by Azure ML backend when the ``data_type`` parameter is not provided. The default is\n",
      " |          False.\n",
      " |      :type is_directory: bool\n",
      " |      :param pipeline_output_name: If provided this output will be available by using\n",
      " |          ``PipelineRun.get_pipeline_output()``. Pipeline output names must be unique in the pipeline.\n",
      " |      :type pipeline_output_name: str\n",
      " |      :param training_output: Defines output for training result. This is needed only for specific trainings which\n",
      " |                              result in different kinds of outputs such as Metrics and Model.\n",
      " |                              For example, :class:`azureml.train.automl.runtime.AutoMLStep`\n",
      " |                              results in metrics and model. You can also define specific training iteration or\n",
      " |                              metric used to get best model.\n",
      " |      :type training_output: azureml.pipeline.core.TrainingOutput, optional\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return __str__.\n",
      " |      \n",
      " |      :return: A string representation of the PipelineData.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      __str__ override.\n",
      " |      \n",
      " |      :return: A string representation of the PipelineData.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  as_dataset(self)\n",
      " |      Promote the intermediate output into a Dataset.\n",
      " |      \n",
      " |      This dataset will exist after the step has executed.\n",
      " |      \n",
      " |      :return: The intermediate output as a Dataset.\n",
      " |      :rtype: azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset\n",
      " |  \n",
      " |  as_download(self, input_name=None, path_on_compute=None, overwrite=None)\n",
      " |      Consume the PipelineData as download.\n",
      " |      \n",
      " |      :param input_name: Use to specify a name for this input.\n",
      " |      :type input_name: str\n",
      " |      :param path_on_compute: The path on the compute to download to.\n",
      " |      :type path_on_compute: str\n",
      " |      :param overwrite: Use to indicate whether to overwrite existing data.\n",
      " |      :type overwrite: bool\n",
      " |      \n",
      " |      :return: The InputPortBinding with this PipelineData as the source.\n",
      " |      :rtype: azureml.pipeline.core.graph.InputPortBinding\n",
      " |  \n",
      " |  as_input(self, input_name)\n",
      " |      Create an InputPortBinding and specify an input name (but use default mode).\n",
      " |      \n",
      " |      :param input_name: Use to specify a name for this input.\n",
      " |      :type input_name: str\n",
      " |      \n",
      " |      :return: The InputPortBinding with this PipelineData as the source.\n",
      " |      :rtype: azureml.pipeline.core.graph.InputPortBinding\n",
      " |  \n",
      " |  as_mount(self, input_name=None)\n",
      " |      Consume the PipelineData as mount.\n",
      " |      \n",
      " |      :param input_name: Use to specify a name for this input.\n",
      " |      :type input_name: str\n",
      " |      \n",
      " |      :return: The InputPortBinding with this PipelineData as the source.\n",
      " |      :rtype: azureml.pipeline.core.graph.InputPortBinding\n",
      " |  \n",
      " |  create_input_binding(self, input_name=None, mode=None, path_on_compute=None, overwrite=None)\n",
      " |      Create input binding.\n",
      " |      \n",
      " |      :param input_name: The name of the input.\n",
      " |      :type input_name: str\n",
      " |      :param mode: The mode to access the PipelineData (\"mount\" or \"download\").\n",
      " |      :type mode: str\n",
      " |      :param path_on_compute: For \"download\" mode, the path on the compute the data will reside.\n",
      " |      :type path_on_compute: str\n",
      " |      :param overwrite: For \"download\" mode, whether to overwrite existing data.\n",
      " |      :type overwrite: bool\n",
      " |      \n",
      " |      :return: The InputPortBinding with this PipelineData as the source.\n",
      " |      :rtype: azureml.pipeline.core.graph.InputPortBinding\n",
      " |  \n",
      " |  get_env_variable_name(self)\n",
      " |      Return the name of the environment variable for this PipelineData.\n",
      " |      \n",
      " |      :return: The environment variable name.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  data_type\n",
      " |      Type of data which will be produced.\n",
      " |      \n",
      " |      :return: The data type name.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  datastore\n",
      " |      Datastore the PipelineData will reside on.\n",
      " |      \n",
      " |      :return: The Datastore object.\n",
      " |      :rtype: azureml.data.azure_storage_datastore.AbstractAzureStorageDatastore or\n",
      " |          azureml.data.azure_data_lake_datastore.AzureDataLakeDatastore\n",
      " |  \n",
      " |  name\n",
      " |      Name of the PipelineData object.\n",
      " |      \n",
      " |      :return: Name.\n",
      " |      :rtype: str\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clean_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
