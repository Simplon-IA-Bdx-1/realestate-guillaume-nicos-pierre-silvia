{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core import Environment\n",
    "#from azureml.data.dataset_consumption_config import DatasetConsumptionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "def_file_store = Datastore(ws, \"workspacefilestore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localenv = Environment(name=\"localenv\")\n",
    "\n",
    "localenv= Environment.from_conda_specification(\"localenv\", './environment.yml')\n",
    "#localenv.docker = False\n",
    "localenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing clean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='annonces_ds')\n",
    "experiment = Experiment(ws, \"CleanTest\")\n",
    "\n",
    "clean_est = Estimator(source_directory='./pipeline_steps', entry_script='clean.py',\n",
    "                script_params= {\"--input\": 'annonces_ds', \"--output\": 'cleantest'},\n",
    "                inputs=[dataset.as_named_input('annonces_ds')],\n",
    "                compute_target='local',\n",
    "                environment_definition=localenv\n",
    "               )\n",
    "\n",
    "run = experiment.submit(clean_est)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/cleantest', output_file_path='./cleantmp.csv')\n",
    "!head ./cleantmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ref = def_blob_store.upload_files(\n",
    "    ['./cleantmp.csv'],\n",
    "    target_path='tmp/clean.csv',\n",
    "    overwrite=True)\n",
    "clean_ref.data_reference_name = \"clean_data\"\n",
    "clean_ref.mode = 'download' # 'download'\n",
    "clean_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(def_blob_store, 'tmp/clean.csv')]\n",
    "clean_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "# # clean_ds = clean_ds.register(workspace=ws,\n",
    "# #                            name='clean_ds',\n",
    "# #                            description='annonces data clean')\n",
    "# clean_ds = DatasetConsumptionConfig('clean_ds', clan_ds, mode='direct', path_on_compute=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_est = Estimator(source_directory='./pipeline_steps',\n",
    "                      entry_script='split.py',\n",
    "                      script_params= {\"--dataset\": \"clean_data\",\n",
    "                                      \"--train\": \"train_ds\",\n",
    "                                      \"--valid\": \"valid_ds\",\n",
    "                                      \"--trainsize\": 400,\n",
    "                                      \"--validsize\": 100},\n",
    "                      #inputs=[clean_ref],\n",
    "                      inputs=[dataset.as_named_input('clean_data')],\n",
    "                      compute_target='local',\n",
    "                      environment_definition=localenv\n",
    "                     )\n",
    "\n",
    "run = experiment.submit(split_est)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/train_ds', output_file_path='./train_dstmp.csv')\n",
    "!head ./cleantmp.csv\n",
    "run.download_file('outputs/valid_ds', output_file_path='./valid_dstmp.csv')\n",
    "!head ./cleantmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ref = def_blob_store.upload_files(\n",
    "    ['./train_dstmp.csv'],\n",
    "    target_path='tmp/train.csv',\n",
    "    overwrite=True)\n",
    "train_ref.data_reference_name = \"train_data\"\n",
    "train_ref.mode = 'download'\n",
    "\n",
    "valid_ref = def_blob_store.upload_files(\n",
    "    ['./valid_dstmp.csv'],\n",
    "    target_path='tmp/valid.csv',\n",
    "    overwrite=True)\n",
    "valid_ref.data_reference_name = \"valid_data\"\n",
    "valid_ref.mode = 'download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(def_blob_store, 'tmp/train.csv')]\n",
    "train_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "\n",
    "datastore_paths = [(def_blob_store, 'tmp/valid.csv')]\n",
    "valid_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_est = Estimator(source_directory='./pipeline_steps', entry_script='train.py',\n",
    "                script_params= {\"--dataset\": \"train_data\",\n",
    "                                \"--model\": \"model.pkl\"},\n",
    "                inputs=[train_ref],\n",
    "                #inputs=[dataset.as_named_input('clean_data')],\n",
    "                compute_target='local',\n",
    "                environment_definition=localenv\n",
    "               )\n",
    "\n",
    "run = experiment.submit(split_est)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='test_model',\n",
    "                           tags={'test': 'test'},\n",
    "                           model_path='outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_est = Estimator(source_directory='.', entry_script='./pipeline_steps/eval.py',\n",
    "                script_params= {\"--dataset\": \"valid_data\",\n",
    "                                \"--model\": \"model.pkl\"},\n",
    "                inputs=[valid_ref],\n",
    "                compute_target='local',\n",
    "                environment_definition=localenv\n",
    "               )\n",
    "\n",
    "run = experiment.submit(eval_est)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
